lambda <- bc$x[which.max(bc$y)]
if(lambda == 0){
model <- lm(log(filter + .0001) ~ test)
} else {
model <- lm(filter**lambda ~ test)
}
mean_filterEffect <- model$coef[[1]] + model$coef[[2]]*mean_testEffect
}
message("computing probabilities")
# compute the probability of the rank of the filter given the mean effect
prob <- sapply(1:m, prob_rank_givenEffect, et = mean_filterEffect,
ey = mean_filterEffect, nrep = nrep, m0 = m0, m1 = m1)
message("finished computing the probabilities")
}
# compute the weights (always right-tailed)------------
message("computing weights")
if(effectType == "continuous"){
wgt = weight_continuous(alpha = alpha, et = mean_testEffect, m = m,
tail = 1, delInterval = delInterval, ranksProb = prob)
} else {
wgt = weight_binary(alpha = alpha, et = mean_testEffect, m = m, m1 = m1,
tail = 1, delInterval = delInterval, ranksProb = prob)
}
message("finished computing the weights")
}
message("comparing pvalues with thresholds")
if(method == "BH"){
padj <- p.adjust(Ordered.pvalue/wgt, method = "BH")
rejections_list = OD[which((padj <= alpha) == TRUE), ]
} else {
rejections_list = OD[which((Ordered.pvalue <= alpha*wgt/m) == TRUE), ]
}
# outputs--------------
if(is.null(ranksProb)){
prob <- NULL
}
n_rejections = dim(rejections_list)[1]
return(list(totalTests = m, propNulls = nullProp,
ranksProb = prob, weight = wgt,
rejections = n_rejections, rejections_list = rejections_list))
}
m = 1000
set.seed(3)
filters = runif(m, min = 0, max = 2.5)          # filter statistics
H = rbinom(m, size = 1, prob = 0.1)             # hypothesis true or false
tests = rnorm(m, mean = H * filters)            # Z-score
pvals = 1 - pnorm(tests)                        # pvalue
# ranks = FLASE to use probability from the normal density
results <- opw(pvalue = pvals, filter = filters, effectType = "continuous", method = "BH")
# ranks = TRUE to use ranks probability
results2 <- opw(pvalue = pvals, filter = filters, effectType = "continuous",
tail = 2, method = "BH")
mod <- lm(log(filters) ~ tests)
et = mean(tests)
ey = mod$coef[[1]] + mod$coef[[2]]*et
results2 <- opw(pvalue = pvals, filter = filters,
mean_filterEffect = ey, mean_testEffect = et, tail = 2,
effectType = "continuous", method = "BH")
# supply the probabilities externally as follows:
library(qvalue)
ranks <- 1:m
nullProp = qvalue(p = pvals, pi0.method = "bootstrap")$pi0
m0 = ceiling(nullProp*m)
m1 = m - m0
probs <- sapply(ranks, prob_rank_givenEffect, et = ey, ey = ey,
nrep = 10000, m0 = m0, m1 = m1)
results3 <- opw(pvalue = pvals, filter = filters, ranksProb = probs,
ranks = FALSE, effectType = "continuous", tail = 2, method = "BH")
wgt <- weight_continuous(alpha = .05, et = et, m = m, ranksProb = probs)
results4 <- opw(pvalue = pvals, filter = filters, weight = wgt,
effectType = "continuous", alpha = .05, method = "BH")
install.packages("DESeq2")
library("ggplot2")
library("airway")
library("DESeq2")
library(gridExtra)
library(cowplot)
library(tibble)
library(qvalue)
library(MASS)
data("airway", package = "airway")
dds <- DESeqDataSet(se = airway, design = ~ cell + dex)
dds <- DESeq(dds)
de_res_air <- as.data.frame(results(dds))
colnames(de_res_air)
dim(de_res_air)
library("OPWeight")
filters = de_res_air$baseMean + .0001  # add a small constant to make all values positive
set.seed(123)
opw_results <- opw(pvalue = de_res_air$pvalue, filter = filters,
alpha = .1, tail = 2, effectType = "continuous", method = "BH")
library("OPWeight")
filters = de_res_air$baseMean + .0001  # add a small constant to make all values positive
set.seed(123)
opw_results <- opw(pvalue = de_res_air$pvalue, filter = filters,
alpha = .1, tail = 2, effectType = "continuous", method = "BH")
names(opw_results)
opw_results$propNulls
opw_results$rejections
m = opw_results$totalTests
testRanks = 1:m
probs = opw_results$ranksProb
weights = opw_results$weight
dat = data.frame(testRanks, probs, weights)
testRanks
m = opw_results$totalTests
testRanks = 1:m
probs = opw_results$ranksProb
weights = opw_results$weight
dat = data.frame(testRanks, probs, weights)
probs
opw <- function(pvalue, filter, weight = NULL, ranksProb = NULL, mean_filterEffect = NULL,
mean_testEffect = NULL, effectType = c("continuous", "binary"),
alpha = .05, nrep = 10000, tail = 1L, delInterval = .0001,
method = c("BH", "BON"), ... )
{
# compute the number of tests------------
m = length(pvalue)
nullProp = qvalue(p = pvalue, pi0.method = "bootstrap")$pi0
m0 = ceiling(nullProp*m)
m1 = m - m0
# formulate a data set-------------
Data = tibble(pvalue, filter)
OD <- Data[order(Data$filter, decreasing=T), ]
Ordered.pvalue <- OD$pvalue
#check whether weight is provided------------
if(!is.null(weight)){
wgt <- weight
} else {
# compute test statistics from the pvalues---------
if(tail == 1){
test <- qnorm(pvalue, lower.tail = FALSE)
} else {
test <- qnorm(pvalue/2, lower.tail = FALSE)
}
test[which(!is.finite(test))] <- NA
# estimate the true alterantive test effect sizes----------------
if(m1 == 0){
test_effect_vec <- 0
} else {
test_effect_vec <-  sort(test, decreasing = TRUE)[1:m1]
}
# estimate the mean test effect size-------------
if(!is.null(mean_testEffect)){
mean_testEffect <- mean_testEffect
} else {
if(effectType == "continuous"){
mean_testEffect <- mean(test_effect_vec, na.rm = TRUE)
} else {
mean_testEffect <- median(test_effect_vec, na.rm = TRUE)
}
}
#check whether filter ranks probability is provided------------
if(!is.null(ranksProb)){
ranksProb <- ranksProb
} else {
# estimate the mean filter effect size------------------
if(!is.null(mean_filterEffect)){
mean_filterEffect <- mean_filterEffect
} else {
# check whether the filters are positive for the box-cox--------
if(any(filter <= 0)){
stop("filter statistics need to be positive")
}
bc <- boxcox(filter ~ test)
lambda <- bc$x[which.max(bc$y)]
if(lambda == 0){
model <- lm(log(filter + .0001) ~ test)
} else {
model <- lm(filter**lambda ~ test)
}
mean_filterEffect <- model$coef[[1]] + model$coef[[2]]*mean_testEffect
}
message("computing probabilities")
# compute the probability of the rank of the filter given the mean effect
ranksProb <- sapply(1:m, prob_rank_givenEffect, et = mean_filterEffect,
ey = mean_filterEffect, nrep = nrep, m0 = m0, m1 = m1)
message("finished computing the probabilities")
}
# compute the weights (always right-tailed)------------
message("computing weights")
if(effectType == "continuous"){
wgt = weight_continuous(alpha = alpha, et = mean_testEffect, m = m,
tail = 1, delInterval = delInterval, ranksProb = ranksProb)
} else {
wgt = weight_binary(alpha = alpha, et = mean_testEffect, m = m, m1 = m1,
tail = 1, delInterval = delInterval, ranksProb = ranksProb)
}
message("finished computing the weights")
}
message("comparing pvalues with thresholds")
if(method == "BH"){
padj <- p.adjust(Ordered.pvalue/wgt, method = "BH")
rejections_list = OD[which((padj <= alpha) == TRUE), ]
} else {
rejections_list = OD[which((Ordered.pvalue <= alpha*wgt/m) == TRUE), ]
}
# outputs--------------
n_rejections = dim(rejections_list)[1]
return(list(totalTests = m, nullProp = nullProp,
ranksProb = ranksProb, weight = wgt,
rejections = n_rejections, rejections_list = rejections_list))
}
results <- opw(pvalue = pvals, filter = filters, effectType = "continuous",
method = "BH")
results
method = "BH")
opw <- function(pvalue, filter, weight = NULL, ranksProb = NULL, mean_filterEffect = NULL,
mean_testEffect = NULL, effectType = c("continuous", "binary"),
alpha = .05, nrep = 10000, tail = 1L, delInterval = .0001,
method = c("BH", "BON"), ... )
{
# compute the number of tests------------
m = length(pvalue)
nullProp = qvalue(p = pvalue, pi0.method = "bootstrap")$pi0
m0 = ceiling(nullProp*m)
m1 = m - m0
# formulate a data set-------------
Data = tibble(pvalue, filter)
OD <- Data[order(Data$filter, decreasing=T), ]
Ordered.pvalue <- OD$pvalue
#check whether weight is provided------------
if(!is.null(weight)){
wgt <- weight
} else {
# compute test statistics from the pvalues---------
if(tail == 1){
test <- qnorm(pvalue, lower.tail = FALSE)
} else {
test <- qnorm(pvalue/2, lower.tail = FALSE)
}
test[which(!is.finite(test))] <- NA
# estimate the true alterantive test effect sizes----------------
if(m1 == 0){
test_effect_vec <- 0
} else {
test_effect_vec <-  sort(test, decreasing = TRUE)[1:m1]
}
# estimate the mean test effect size-------------
if(!is.null(mean_testEffect)){
mean_testEffect <- mean_testEffect
} else {
if(effectType == "continuous"){
mean_testEffect <- mean(test_effect_vec, na.rm = TRUE)
} else {
mean_testEffect <- median(test_effect_vec, na.rm = TRUE)
}
}
#check whether filter ranks probability is provided------------
if(!is.null(ranksProb)){
ranksProb <- ranksProb
} else {
# estimate the mean filter effect size------------------
if(!is.null(mean_filterEffect)){
mean_filterEffect <- mean_filterEffect
} else {
# check whether the filters are positive for the box-cox--------
if(any(filter <= 0)){
stop("filter statistics need to be positive")
}
bc <- boxcox(filter ~ test)
lambda <- bc$x[which.max(bc$y)]
if(lambda == 0){
model <- lm(log(filter + .0001) ~ test)
} else {
model <- lm(filter**lambda ~ test)
}
mean_filterEffect <- model$coef[[1]] + model$coef[[2]]*mean_testEffect
}
message("computing probabilities")
# compute the probability of the rank of the filter given the mean effect
ranksProb <- sapply(1:m, prob_rank_givenEffect, et = mean_filterEffect,
ey = mean_filterEffect, nrep = nrep, m0 = m0, m1 = m1)
message("finished computing the probabilities")
}
# compute the weights (always right-tailed)------------
message("computing weights")
if(effectType == "continuous"){
wgt = weight_continuous(alpha = alpha, et = mean_testEffect, m = m,
tail = 1, delInterval = delInterval, ranksProb = ranksProb)
} else {
wgt = weight_binary(alpha = alpha, et = mean_testEffect, m = m, m1 = m1,
tail = 1, delInterval = delInterval, ranksProb = ranksProb)
}
message("finished computing the weights")
}
message("comparing pvalues with thresholds")
if(method == "BH"){
padj <- p.adjust(Ordered.pvalue/wgt, method = "BH")
rejections_list = OD[which((padj <= alpha) == TRUE), ]
} else {
rejections_list = OD[which((Ordered.pvalue <= alpha*wgt/m) == TRUE), ]
}
# outputs--------------
n_rejections = dim(rejections_list)[1]
return(list(totalTests = m, nullProp = nullProp,
ranksProb = ranksProb, weight = wgt,
rejections = n_rejections, rejections_list = rejections_list))
}
results <- opw(pvalue = pvals, filter = filters, effectType = "continuous",
method = "BH")
m = 1000
set.seed(3)
filters = runif(m, min = 0, max = 2.5)          # filter statistics
H = rbinom(m, size = 1, prob = 0.1)             # hypothesis true or false
tests = rnorm(m, mean = H * filters)            # Z-score
pvals = 1 - pnorm(tests)                        # pvalue
# general use
results <- opw(pvalue = pvals, filter = filters, effectType = "continuous",
method = "BH")
# supply the mean effects for both the filters and the tests externally
mod <- lm(log(filters) ~ tests)
et = mean(tests)
ey = mod$coef[[1]] + mod$coef[[2]]*et
results2 <- opw(pvalue = pvals, filter = filters,
mean_filterEffect = ey, mean_testEffect = et, tail = 2,
effectType = "continuous", method = "BH")
' results
results
results2
mod <- lm(log(filters) ~ tests)
library(qvalue)
ranks <- 1:m
nullProp = qvalue(p = pvals, pi0.method = "bootstrap")$pi0
m0 = ceiling(nullProp*m)
m1 = m - m0
probs <- sapply(ranks, prob_rank_givenEffect, et = ey, ey = ey,
nrep = 10000, m0 = m0, m1 = m1)
results3 <- opw(pvalue = pvals, filter = filters, ranksProb = probs,
ranks = FALSE, effectType = "continuous", tail = 2, method = "BH")
results3
wgt <- weight_continuous(alpha = .05, et = et, m = m, ranksProb = probs)
results4 <- opw(pvalue = pvals, filter = filters, weight = wgt,
effectType = "continuous", alpha = .05, method = "BH")
results4
rm(list=ls())
library("OPWeight")
filters = de_res_air$baseMean + .0001  # add a small constant to make all values positive
library("ggplot2")
library("airway")
library("DESeq2")
library(gridExtra)
library(cowplot)
library(tibble)
library(qvalue)
library(MASS)
data("airway", package = "airway")
dds <- DESeqDataSet(se = airway, design = ~ cell + dex)
dds <- DESeq(dds)
de_res_air <- as.data.frame(results(dds))
colnames(de_res_air)
dim(de_res_air)
library("OPWeight")
filters = de_res_air$baseMean + .0001  # add a small constant to make all values positive
set.seed(123)
opw_results <- opw(pvalue = de_res_air$pvalue, filter = filters,
alpha = .1, tail = 2, effectType = "continuous", method = "BH")
names(opw_results)
opw_results$nullProp
library(OPWeight)
names(opw_results)
opw_results$nullProp
library(OPWeight)
opw
knitr::opts_chunk$set(tidy = FALSE, cache = TRUE, autodep = TRUE)
m = 1000
set.seed(3)
baseMean = runif(m, min = 0, max = 2.5)          # filter statistics
H = rbinom(m, size = 1, prob = 0.1)             # hypothesis true or false
stat = rnorm(m, mean = H * baseMean)            # Z-score
pvalue = 1 - pnorm(tests)
m = 1000
set.seed(3)
baseMean = runif(m, min = 0, max = 2.5)          # filter statistics
H = rbinom(m, size = 1, prob = 0.1)             # hypothesis true or false
stat = rnorm(m, mean = H * baseMean)            # Z-score
pvalue = 1 - pnorm(stat)
de_res_air = tibble(basemean, stat, pvalue)
m = 1000
set.seed(3)
baseMean = runif(m, min = 0, max = 2.5)          # filter statistics
H = rbinom(m, size = 1, prob = 0.1)             # hypothesis true or false
stat = rnorm(m, mean = H * baseMean)            # Z-score
pvalue = 1 - pnorm(stat)
de_res_air = tibble(baseMean, stat, pvalue)
colnames(de_res_air)
dim(de_res_air)
library("OPWeight")
filters = de_res_air$baseMean + .0001  # add a small constant to make all values positive
set.seed(123)
opw_results <- opw(pvalue = de_res_air$pvalue, filter = filters,
alpha = .1, tail = 2, effectType = "continuous", method = "BH")
names(opw_results)
opw_results$nullProp
opw_results$rejections
m = opw_results$totalTests
testRanks = 1:m
probs = opw_results$ranksProb
weights = opw_results$weight
dat = data.frame(testRanks, probs, weights)
p_prob = ggplot(dat, aes(x = testRanks, y = probs)) + geom_point(size = .5, col="firebrick4") +
labs(y = "p(rank | effect)")
library("ggplot2")
library("airway")
library("DESeq2")
library(gridExtra)
library(cowplot)
library(tibble)
library(qvalue)
library(MASS)
m = opw_results$totalTests
testRanks = 1:m
probs = opw_results$ranksProb
weights = opw_results$weight
dat = data.frame(testRanks, probs, weights)
p_prob = ggplot(dat, aes(x = testRanks, y = probs)) + geom_point(size = .5, col="firebrick4") +
labs(y = "p(rank | effect)")
p_weight = ggplot(dat, aes(x = testRanks, y = weights)) + geom_point(size = .5, col="firebrick4")
plot_grid(p_prob, p_weight, labels = c("A", "B"))
dat
m = opw_results$totalTests
testRanks = 1:m
probs = opw_results$ranksProb
weights = opw_results$weight
dat = tibble(testRanks, probs, weights)
p_prob = ggplot(dat, aes(x = testRanks, y = probs)) + geom_point(size = .5, col="firebrick4") +
labs(y = "p(rank | effect)")
p_weight = ggplot(dat, aes(x = testRanks, y = weights)) + geom_point(size = .5, col="firebrick4")
plot_grid(p_prob, p_weight, labels = c("A", "B"))
dat
Data <- tibble(pval=de_res_air$pvalue, filter=de_res_air$baseMean)
barlines <- "#1F3552"
hist_pval <- ggplot(Data, aes(x = Data$pval)) +
geom_histogram(colour = barlines, fill = "#4281AE")+
labs(x = "pvalues")
hist_filter <- ggplot(Data, aes(x = Data$filter)) +
geom_histogram( colour = barlines, fill = "#4274AE") +
labs(x = "filter statistics")
pval_filter <- ggplot(Data, aes(x = rank(-Data$filter), y = -log10(pval))) +
geom_point()+
labs(x = "ranks of filters", y = "-log(pvalue)")
p_ecdf <- ggplot(Data, aes(x = pval)) +
stat_ecdf(geom = "step")+
labs(x = "pvalues", title="empirical cumulative distribution")+
theme(plot.title = element_text(size = rel(.7)))
p <- plot_grid(hist_pval, hist_filter, pval_filter, p_ecdf,
labels = c("A", "B", "c", "D"), ncol = 2)
# now add the title
title <- ggdraw() + draw_label("Airway: data summary")
plot_grid(title, p, ncol = 1, rel_heights=c(0.1, 1))
# initial stage--------
pvals = de_res_air$pvalue
tests = qnorm(pvals/2, lower.tail = FALSE)
filters = de_res_air$baseMean + .0001
# formulate a data set-------------
Data = tibble(pvals, filters)
OD <- Data[order(Data$filters, decreasing=T), ]
Ordered.pvalue <- OD$pvals
# estimate the true null and alternative test sizes------
m = length(Data$pvals); m
null = qvalue(Data$pvals, pi0.method="bootstrap")$pi0; null
m0 = ceiling(null*m); m0
m1 = m - m0; m1
# fit box-cox regression
#--------------------------------
bc <- boxcox(filters ~ tests)
lambda <- bc$x[which.max(bc$y)]; lambda
model <- lm(filters^lambda ~ tests)
# If lambda = 0. use log-transformation
# model <- lm(log(Data$filters) ~ Data$tests)
# etimated test efects of the true altrnatives------------
test_effect = if(m1 == 0) {0
} else {sort(tests, decreasing = T)[1:m1]}		# two-tailed test
# for the continuous effects etimated mean effects
mean_testEffect = mean(test_effect, na.rm = T)
mean_testEffect
mean_filterEffect = model$coef[[1]] + model$coef[[2]]*mean_testEffect
mean_filterEffect
# # for the binary effects estiamted median effects
# mean_testEffect = median(test_effect, na.rm = T)
# mean_testEffect
# mean_filterEffect = model$coef[[1]] + model$coef[[2]]*mean_testEffect
# mean_filterEffect
set.seed(123)
prob_cont <- sapply(1:m, prob_rank_givenEffect, et = mean_filterEffect,
ey = mean_filterEffect, nrep = 10000, m0 = m0, m1 = m1)
wgt <- weight_continuous(alpha = .1, et = mean_testEffect, m = m, ranksProb = prob_cont)
alpha = .1
padj <- p.adjust(Ordered.pvalue/wgt, method = "BH")
rejections_list = OD[which((padj <= alpha) == TRUE), ]
n_rejections = dim(rejections_list)[1]
n_rejections
opw_results2 <- opw(pvalue = pvals, filter = filters, weight = wgt,
effectType = "continuous", alpha = .1, method = "BH")
library("OPWeight")
filters = de_res_air$baseMean + .0001  # add a small constant to make all values positive
set.seed(123)
opw_results <- opw(pvalue = de_res_air$pvalue, filter = filters,
alpha = .1, tail = 2, effectType = "continuous", method = "BH")
library(OPWeight)
library(OPWeight)
library(OPWeight)
library(OPWeight)
build()
library(devtools)
build()
use_travis()
?use_travis
library(devtools)
?use_travis
use_travis()
use_travis()
